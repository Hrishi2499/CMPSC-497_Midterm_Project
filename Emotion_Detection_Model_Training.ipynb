{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RmNmuOAjbrSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2Ym_D9Wq3VI"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/CSE_497/Midterm_Project/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Dependencies"
      ],
      "metadata": {
        "id": "aBAPC3MvKa2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import train_test_split,KFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adamax, Adam\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.layers import BatchNormalization, Concatenate\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, SpatialDropout1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, ReLU"
      ],
      "metadata": {
        "id": "25eOwLZZq9Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset"
      ],
      "metadata": {
        "id": "IIM-uIMBKjVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('emotion_data_alt.csv')\n",
        "len(df)"
      ],
      "metadata": {
        "id": "LNb32pCpUMpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing"
      ],
      "metadata": {
        "id": "_5Ig22T3K1XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer= WordNetLemmatizer()\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"coz\":\"because\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "def _get_contractions(contraction_dict):\n",
        "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    return contraction_dict, contraction_re\n",
        "\n",
        "contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "\n",
        "def replace_contractions(text): # Expanding contraction\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "  url_pattern = re.compile(r'https?://\\S+|www\\.\\S+') # Remove URLS\n",
        "  text = url_pattern.sub(r'', text)\n",
        "\n",
        "  text = replace_contractions(text)\n",
        "\n",
        "  text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text) # Remove Punctuations\n",
        "  text = text.replace('؛',\"\", )\n",
        "\n",
        "  text = re.sub('\\s+', ' ', text) # Remove Empty spaces\n",
        "  text =  \" \".join(text.split())\n",
        "\n",
        "  text = text.split()\n",
        "  text = [y.lower().strip() for y in text] # Lower case\n",
        "\n",
        "  lemmatized = []\n",
        "  for word in text:\n",
        "    if( word not in stop_words) and (word not in string.punctuation) and (not word.isdigit()): # Remove stopwords and numbers\n",
        "      word = lemmatizer.lemmatize(word)  # Lemmatize the words\n",
        "      lemmatized.append(word)\n",
        "\n",
        "  return \" \".join(lemmatized)\n",
        "\n",
        "\n",
        "def normalize_sentence(sentence):\n",
        "    sentence = preprocess_text(sentence)\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "mPMPjNyKsVGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the pre processing to the dataset"
      ],
      "metadata": {
        "id": "FK06N7lfLSDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.Text = df.Text.apply(lambda text : normalize_sentence(text))"
      ],
      "metadata": {
        "id": "1F-L8S3BsoQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset into train, test and validation set"
      ],
      "metadata": {
        "id": "OLYf5NxALaKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['Text']\n",
        "y = df['Emotion']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "q5baW6PRsqxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding the labels using LabelEncoder"
      ],
      "metadata": {
        "id": "WkEwlu2GLjDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "y_val = le.transform(y_val)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "y_val = to_categorical(y_val)"
      ],
      "metadata": {
        "id": "REhfU6xytCeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing the data"
      ],
      "metadata": {
        "id": "9ZyenMAaLpNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(oov_token='UNK')\n",
        "tokenizer.fit_on_texts(pd.concat([X_train, X_test, X_val], axis=0))"
      ],
      "metadata": {
        "id": "nJ1L_ZuXtFSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting text to sequences"
      ],
      "metadata": {
        "id": "oasWVKclLweO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
        "sequences_val = tokenizer.texts_to_sequences(X_val)"
      ],
      "metadata": {
        "id": "EvA9fay9tMwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = max([len(t.split()) for t in df['Text']])\n",
        "maxlen"
      ],
      "metadata": {
        "id": "M4EXFwLItROI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding the data values to make it uniform"
      ],
      "metadata": {
        "id": "8lL6AAysLzkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(sequences_train, maxlen=maxlen, truncating='post')\n",
        "X_test = pad_sequences(sequences_test, maxlen=maxlen, truncating='post')\n",
        "X_val = pad_sequences(sequences_val, maxlen=maxlen, truncating='post')\n",
        "\n",
        "vocabSize = len(tokenizer.index_word) + 1\n",
        "print(f\"Vocabulary size = {vocabSize}\")"
      ],
      "metadata": {
        "id": "Ihmip0yOtUr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading GloVe dataset"
      ],
      "metadata": {
        "id": "943LDUtAL4YG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "metadata": {
        "id": "76FCv9qKRSLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = vocabSize\n",
        "embedding_dim = 200\n",
        "embeddings_index = {}\n",
        "\n",
        "with open('./glove.6B.200d.txt') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
        "\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "1acViDQDtjJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early Stopping callback configuration"
      ],
      "metadata": {
        "id": "2NbCFS6dL_bO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callback = EarlyStopping(monitor=\"val_loss\",patience=4, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "bQp5ExgUt796"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the CNN model"
      ],
      "metadata": {
        "id": "FV2rKb6VMEdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cnn model\n",
        "CNNmodel = Sequential()\n",
        "CNNmodel.add(Embedding(vocabSize, 200, weights=[embedding_matrix], input_length=maxlen, trainable=False, input_shape = X_train[0].shape))\n",
        "\n",
        "CNNmodel.add(Conv1D(256, 5, activation='leaky_relu'))\n",
        "CNNmodel.add(BatchNormalization())\n",
        "\n",
        "CNNmodel.add(Conv1D(256, 5, activation='relu'))\n",
        "CNNmodel.add(BatchNormalization())\n",
        "\n",
        "CNNmodel.add(GlobalMaxPooling1D())\n",
        "\n",
        "CNNmodel.add(Dense(256, activation='relu'))\n",
        "CNNmodel.add(BatchNormalization())\n",
        "CNNmodel.add(Dropout(0.5))\n",
        "\n",
        "CNNmodel.add(Dense(128, activation='relu'))\n",
        "CNNmodel.add(BatchNormalization())\n",
        "CNNmodel.add(Dropout(0.3))\n",
        "\n",
        "CNNmodel.add(Dense(6, activation='softmax'))\n",
        "\n",
        "CNNmodel.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "CNNmodel.summary()"
      ],
      "metadata": {
        "id": "AFoXrFnpVDfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the CNN model"
      ],
      "metadata": {
        "id": "fXhN6l9FMG5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model\n",
        "with tf.device('/device:GPU:0'):\n",
        "  performance = CNNmodel.fit(X_train,\n",
        "                      y_train,\n",
        "                      validation_data=(X_val, y_val),\n",
        "                      verbose=1,\n",
        "                      batch_size=128,\n",
        "                      epochs=30,\n",
        "                      callbacks=[callback]\n",
        "                    )"
      ],
      "metadata": {
        "id": "ghBtqfLcVDZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the CNN model"
      ],
      "metadata": {
        "id": "dEFBKNmVMJeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = CNNmodel.predict(X_test)\n",
        "y_pred = predicted.argmax(axis=-1)\n",
        "\n",
        "print(classification_report(y_test.argmax(axis=-1), y_pred))"
      ],
      "metadata": {
        "id": "edynCf6lVDOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the CNN model"
      ],
      "metadata": {
        "id": "nIOUj8ORMU_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('CNNmodel_alt.pkl', 'wb') as f:\n",
        "    pickle.dump(CNNmodel, f)"
      ],
      "metadata": {
        "id": "lgyeGhNhQ2ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the SimpleRNN model"
      ],
      "metadata": {
        "id": "9gCGc5etMahW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adam = Adam(learning_rate=0.005)\n",
        "\n",
        "simpleRNN = Sequential()\n",
        "simpleRNN.add(Embedding(vocabSize, 200, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=False, input_shape=X_train[0].shape))\n",
        "\n",
        "simpleRNN.add(SimpleRNN(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
        "simpleRNN.add(SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "simpleRNN.add(Dense(6, activation='softmax'))\n",
        "\n",
        "simpleRNN.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "simpleRNN.summary()"
      ],
      "metadata": {
        "id": "BLterZ31Km60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the SimpleRNN model"
      ],
      "metadata": {
        "id": "RU0GaPR5MeNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model\n",
        "with tf.device('/device:GPU:0'):\n",
        "  history = simpleRNN.fit(X_train,\n",
        "                      y_train,\n",
        "                      validation_data=(X_val, y_val),\n",
        "                      verbose=1,\n",
        "                      batch_size=128,\n",
        "                      epochs=5,\n",
        "                      callbacks=[callback]\n",
        "                    )"
      ],
      "metadata": {
        "id": "9Vd8_aS2Kntz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the SimpleRNN model"
      ],
      "metadata": {
        "id": "4qY-NWbiMhZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = simpleRNN.predict(X_test)\n",
        "y_pred = predicted.argmax(axis=-1)\n",
        "\n",
        "print(classification_report(y_test.argmax(axis=-1), y_pred))"
      ],
      "metadata": {
        "id": "QnYw8h43KnrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the SimpleRNN model"
      ],
      "metadata": {
        "id": "v0DN9LSAMqDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('SimpleRNN_alt.pkl', 'wb') as f:\n",
        "    pickle.dump(simpleRNN, f)"
      ],
      "metadata": {
        "id": "WFlzoYqkKnou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the BiLSTM model"
      ],
      "metadata": {
        "id": "1pYcebSzMsee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adam = Adam(learning_rate=0.005)\n",
        "\n",
        "modelRNN = Sequential()\n",
        "modelRNN.add(Embedding(vocabSize, 200, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=False, input_shape=X_train[0].shape))\n",
        "\n",
        "modelRNN.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "modelRNN.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "modelRNN.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
        "\n",
        "modelRNN.add(Dense(6, activation='softmax'))\n",
        "\n",
        "modelRNN.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "modelRNN.summary()"
      ],
      "metadata": {
        "id": "q0Wwj5Rftq2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the BiLSTM model"
      ],
      "metadata": {
        "id": "s8fpIrf6MvpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model\n",
        "with tf.device('/device:GPU:0'):\n",
        "  history = modelRNN.fit(X_train,\n",
        "                      y_train,\n",
        "                      validation_data=(X_val, y_val),\n",
        "                      verbose=1,\n",
        "                      batch_size=128,\n",
        "                      epochs=5,\n",
        "                      callbacks=[callback]\n",
        "                    )"
      ],
      "metadata": {
        "id": "X100iNkyIIv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the BiLSTM model"
      ],
      "metadata": {
        "id": "GaF2ReHHM3B-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = modelRNN.predict(X_test)\n",
        "y_pred = predicted.argmax(axis=-1)\n",
        "\n",
        "print(classification_report(y_test.argmax(axis=-1), y_pred))"
      ],
      "metadata": {
        "id": "e7pS56nquGTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the confusion matrix"
      ],
      "metadata": {
        "id": "cFPqnR5gNwWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "\n",
        "confusion_matrix = confusion_matrix(y_test.argmax(axis=-1), y_pred)\n",
        "cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels=le.inverse_transform([0, 1, 2, 3, 4, 5]))\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RrHXzP0-Ns6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the BiLSTM model"
      ],
      "metadata": {
        "id": "5qsLC6UdM5o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('RNNmodel_alt.pkl', 'wb') as f:\n",
        "    pickle.dump(modelRNN, f)"
      ],
      "metadata": {
        "id": "nqb10AB_RQ4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the Combined (CNN + BiLSTM) model"
      ],
      "metadata": {
        "id": "r4QYHSKhNEq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CombinedModel = Sequential()\n",
        "CombinedModel.add(Embedding(vocabSize,\n",
        "                        embedding_dim,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=maxlen,\n",
        "                        trainable=False, input_shape = X_train[0].shape))\n",
        "\n",
        "CombinedModel.add(SpatialDropout1D(0.2))\n",
        "\n",
        "# CNN layers\n",
        "CombinedModel.add(Conv1D(256, 5, activation='relu', padding='same'))\n",
        "CombinedModel.add(BatchNormalization())\n",
        "CombinedModel.add(Conv1D(256, 5, activation='relu', padding='same'))\n",
        "CombinedModel.add(BatchNormalization())\n",
        "CombinedModel.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# RNN layers\n",
        "CombinedModel.add(Bidirectional(LSTM(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "CombinedModel.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
        "CombinedModel.add(Dense(128, activation='relu'))\n",
        "CombinedModel.add(BatchNormalization())\n",
        "CombinedModel.add(Dropout(0.3))\n",
        "\n",
        "# Output layer\n",
        "CombinedModel.add(Dense(6, activation='softmax'))\n",
        "\n",
        "CombinedModel.summary()\n",
        "CombinedModel.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "c0WkrVTEuJWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the combined model"
      ],
      "metadata": {
        "id": "gKLOqHu8NMKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model\n",
        "with tf.device('/device:GPU:0'):\n",
        "  history = CombinedModel.fit(X_train,\n",
        "                      y_train,\n",
        "                      validation_data=(X_val, y_val),\n",
        "                      verbose=1,\n",
        "                      batch_size=128,\n",
        "                      epochs=15,\n",
        "                      callbacks=[callback]\n",
        "                    )"
      ],
      "metadata": {
        "id": "3RMdUvovP-W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the combined model"
      ],
      "metadata": {
        "id": "0l5eKkNTNPxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = CombinedModel.predict(X_test)\n",
        "y_pred = predicted.argmax(axis=-1)\n",
        "\n",
        "print(classification_report(y_test.argmax(axis=-1), y_pred))"
      ],
      "metadata": {
        "id": "rH8TtW-QQBCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the combined model"
      ],
      "metadata": {
        "id": "q-TkUv-4NXdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('CombinedModel_alt.pkl', 'wb') as f:\n",
        "    pickle.dump(CombinedModel, f)"
      ],
      "metadata": {
        "id": "36bWNyJuRmeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying the confusion matrix"
      ],
      "metadata": {
        "id": "BYzRbqDYN2ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "\n",
        "confusion_matrix = confusion_matrix(y_test.argmax(axis=-1), y_pred)\n",
        "cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels=le.inverse_transform([0, 1, 2, 3, 4, 5]))\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FR8ILmWiaLxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the combined model"
      ],
      "metadata": {
        "id": "VyIdEeU9N7Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('CombinedModel_alt.pkl', 'rb') as f:\n",
        "    CombinedModel = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "OoUbooP6CXNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, fine tuning the Combined Model to find the ideal parameters"
      ],
      "metadata": {
        "id": "mL2EcP4cOAI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras scikit-learn==1.3.1"
      ],
      "metadata": {
        "id": "FbJx6nRSUNPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing GridSearchCV with a parameter grid to find optimal hyper-parameters"
      ],
      "metadata": {
        "id": "Y5L7tQTpOTXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "# Define a function to create the model (required for KerasClassifier)\n",
        "def create_model(filters=128, kernel_size=5, dropout_rate=0.5, learning_rate=0.001, act='relu'):\n",
        "    CombinedModel = Sequential()\n",
        "    CombinedModel.add(Embedding(vocabSize,\n",
        "                            embedding_dim,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=maxlen,\n",
        "                            trainable=False, input_shape = X_train[0].shape))\n",
        "\n",
        "    # Spatial Dropout\n",
        "    CombinedModel.add(SpatialDropout1D(dropout_rate))\n",
        "\n",
        "    # CNN layers\n",
        "    CombinedModel.add(Conv1D(256, kernel_size, activation=act, padding='same'))\n",
        "    CombinedModel.add(BatchNormalization())\n",
        "    CombinedModel.add(Conv1D(filters, kernel_size, activation=act, padding='same'))\n",
        "\n",
        "    CombinedModel.add(BatchNormalization())\n",
        "    CombinedModel.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    CombinedModel.add(GlobalMaxPooling1D())\n",
        "\n",
        "    # RNN layers\n",
        "    CombinedModel.add(Bidirectional(LSTM(filters, dropout=dropout_rate, return_sequences=True, recurrent_dropout=0.2)))\n",
        "    CombinedModel.add(Bidirectional(LSTM(filters, dropout=dropout_rate, recurrent_dropout=0.2)))\n",
        "    CombinedModel.add(Dense(128, activation=act))\n",
        "    CombinedModel.add(BatchNormalization())\n",
        "    CombinedModel.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer\n",
        "    CombinedModel.add(Dense(6, activation='softmax'))\n",
        "\n",
        "    CombinedModel.summary()\n",
        "    CombinedModel.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
        "    return CombinedModel\n",
        "\n",
        "# Wrap the Keras model\n",
        "model = KerasClassifier(create_model, epochs=5, batch_size=128)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'model__filters': [128, 256],\n",
        "    'model__kernel_size': [3, 5],\n",
        "    'model__dropout_rate': [0.3, 0.5],\n",
        "    'model__act': ['relu', 'leaky_relu']\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1)\n",
        "\n",
        "# Perform the grid search\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Print the results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "id": "3iHaq39yrWnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ggx9IAJtTyP_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}